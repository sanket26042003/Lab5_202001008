# -*- coding: utf-8 -*-
"""Sanket_Doshi_A3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pVw3cEezh42p0kutRSdelQUPysdqHG29
"""

import pandas as pd
import numpy as np
import random
import math
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
from google.colab import files

"""**1. Generate the data**"""

X = []
for i in range(0,50):
  X.append(random.randint(-10, 10))

Y = []
for i in range(0,50):
  Y.append(X[i] + 2*math.sin(1.5*X[i]) + np.random.normal(0, 2, 1))

"""**2. Visualizing the data**"""

ax = plt.axes()
ax.scatter(X,Y,c = 'r')
legend_1 = mpatches.Patch(label = 'data points',color = 'r')
ax.legend(handles=[legend_1],loc = 'best')
ax.set_title(f'Data points plotting')
ax.set_xlabel('$x$')
ax.set_ylabel('$y = x + 2 * sin(1.5x)$')
plt.show()

"""*   Fitting a line through the data points is not sufficient, we need a higher degree hypothesis function for this data set.

**3. Gradient descent**
"""

def plot3D(x, y, z, colorMap = 'viridis', ax = None, title = 'Plot of $f(w)$'):
  """Generate a 3D-Surface plot based on given x, y, z grid co-ordinates"""
  #generate a 2D grid for our (w1, w2) pairs
  X, Y = np.meshgrid(x, y)

  #plotting the 3D surface
  
  if ax != None:
    ax.plot_surface(X, Y, z, cmap=colorMap)
  else:
    ax = plt.axes( projection='3d' )
    ax.plot_surface(X, Y, z, cmap=colorMap)

  ax.set_title(title, fontsize = 20)
  ax.set_xlabel('$w_1$', fontsize = 15)
  ax.set_ylabel('$w_2$', fontsize = 15)
  ax.set_zlabel('$f(w)$', fontsize = 15)

def f(X, xRange = 10.0, yRange = 10.0, increment = 0.1):
  #w1, w2 are the two individual vectors for w
  w1 = np.arange( -xRange, xRange, increment )
  w2 = np.arange( -yRange, yRange, increment )

  #Value of cost function (2D array) for each (w1, w2) pair
  val = np.zeros( ( w1.size, w2.size ) )

  for i in range (0,50):
    cx = 0
    for j in w1:
      cy = 0
      for k in w2:
        val[cx, cy]+=((X[i-1] + 2*math.sin(1.5*X[i-1]) - (j*(1/(1+math.exp(-X[i-1]))) + k))**2)
        #val[cx, cy]+=(j*(1/(1+math.exp(-X[i-1]))) + k)
        cy+=1
      cx+=1

  return w1, w2, val

def diffW0(w0, w1, x):
  return ((-2/50)*(x + 2*math.sin(1.5*x) - w1 - w0*(1/(1+math.exp(-x))))*(1/(1+math.exp(-x))))

def diffw1(w0, w1, x):
  return ((-2/50)*(x + 2*math.sin(1.5*x) - w1 - w0*(1/(1+math.exp(-x)))))

def gradientDescent(X, w1_initial, w2_initial, alpha, recordAt = 100, max_iterations = 100000):
  xEstimate = w1_initial
  yEstimate = w2_initial

  learningRate = alpha

  episilon1 = 1e10
  episilon2 = 1e10

  episilon = 1e10

  t = max_iterations

  xHistory = []; yHistory = []

  while (episilon1 >= 1e-5 or episilon2 >= 1e-5):
    if t % recordAt == 0:
      xHistory.append(xEstimate)
      yHistory.append(yEstimate)

    if t <= 0:
      break

    newX = xEstimate - learningRate * diffW0(xEstimate, yEstimate, i)
    newY = yEstimate - learningRate * diffw1(xEstimate, yEstimate, i)

    episilon1 = abs(newX - xEstimate)
    episilon2 = abs(newY - yEstimate)

    # episilon = abs(cost_function(newX, newY) - cost_function(xEstimate, yEstimate))

    xEstimate = newX
    yEstimate = newY

    # print(f"{xEstimate}, {yEstimate}")

    t -= 1
  
  xHistory.append(xEstimate)
  yHistory.append(yEstimate)

  return xHistory, yHistory

fig, ax = plt.subplots(1, 1, figsize=(25, 15), subplot_kw=dict(projection='3d'))

xHistory = []
yHistory = []
valueoff = []
w0_initial = -1
w1_initial = -1

for i in range(1,50):
  [x_temp, y_temp] = gradientDescent(X[i-1],w0_initial, w1_initial, 0.0001, recordAt = 1)

  if i == 1:
    for j in range(0, len(x_temp)):
      xHistory.append(x_temp[j])
      yHistory.append(y_temp[j])
      valueoff.append((X[i-1] + 2*math.sin(1.5*X[i-1]) - (xHistory[j]*(1/(1+math.exp(-X[i-1]))) + yHistory[j])) ** 2)

      if j == (len(x_temp)-1):
        w0_initial = x_temp[j]
        w1_initial = y_temp[j]
  else:
    for j in range(0, len(x_temp)):
      xHistory[j]+=x_temp[j]
      yHistory[j]+=y_temp[j]
      valueoff[j]+=((X[i-1] + 2*math.sin(1.5*X[i-1]) - (xHistory[j]*(1/(1+math.exp(-X[i-1]))) + yHistory[j])))

      if j == (len(x_temp)-1):
        w0_initial = x_temp[j]
        w1_initial = y_temp[j]

for i in range(0, len(xHistory)):
  xHistory[i]/=50
  yHistory[i]/=50
  valueoff[i]/=50

print("W1 = " + str(xHistory[-1]))
print("W0 = " + str(yHistory[-1]))

[x, y, z] = f(X)
plot3D(x, y, z, colorMap = 'GnBu', ax=ax, title = f"Learning Rate : {0.01}")

#ax.scatter(xHistory,yHistory,valueoff,c = 'b')
ax.scatter(xHistory, yHistory, valueoff, s = 40, c='black')

"""**4. Optimal model capacity**

**5. Bias and Variance**
"""

N = 200 # Test set size
L = 100 # Number of datasets
n = 500 # Number of data points

x = np.zeros((L, n))

for i in range(0,L):
  z = []
  z = np.random.choice(X, 500)
  x[i] = z

y = np.zeros((L, n))

for i in range(0,L):
  for j in range(0,n):
    y[i][j] = x[i][j] + 2*math.sin(1.5*x[i][j]) + np.random.normal(0, 2, 1)

y_xn = np.zeros(N)

for j in range(0, N):
  for i in range(0, L):
    y_xn[j]+=y[i][j]
  y_xn[j]/=100

y_hat = np.zeros((L, n))

for i in range(0, L):
  for j in range(0, n):
    y_hat[i][j] = xHistory[-1]*(1/(1 + math.exp(-x[i][j]))) + yHistory[-1]

y_hat_xn = np.zeros(N)

for j in range(0, N):
  for i in range(0, L):
    y_hat_xn[j]+=y_hat[i][j]
  y_hat_xn[j]/=100

bias_2 = 0

for i in range(0, N):
  bias_2+=((y_hat_xn[i] - y_xn[i]) ** 2)
bias_2/=N

print("Bias^2 = " + str(bias_2))

variance = 0

for i in range(0, N):
  for j in range(0, L):
    variance+=((y[j][i] - y_hat_xn[i]) ** 2)
  variance/=L
variance/=N

print("Variance = " + str(variance))

"""Polynomial Function"""

N=50 # number of data points
x=np.linspace(-10,10,50)

# generating signal
y=x+2*np.sin(1.5*x)

#adding gaussian noise with mean 0 and variance 2 to the signal
mean=0
var=2
gaussian_noise=np.random.normal(mean,var,N) 

y_noise = y+gaussian_noise

ax = plt.axes()
ax.scatter(x,y_noise,c = 'r')
legend_1 = mpatches.Patch(label = 'data points',color = 'r')
ax.legend(handles=[legend_1],loc = 'best')
ax.set_title(f'Data points plotting')
ax.set_xlabel('$x$')
ax.set_ylabel('$y = x + 2 * sin(1.5x)$')
plt.show()

#defining polynomial function
#input x = 1xN
#output f= (j+1)xN
def phi_j(x,j):
  N=x.shape[1]
  f=np.zeros((j+1,N),dtype='float')
  for i in range(j+1):
    f[i,:]=(np.power(x,i))
  return f

# defining cost function
def cost(phi_x,y,w):
  N=phi_x.shape[1]
  # print(w.T.shape,phi_x.shape,np.dot(w.T,phi_x).shape,y.shape)
  cost=np.sum(np.square(y-np.dot(w.T,phi_x)))/N
  return cost

def get_weights(x,y):
  x=x.T
  w=np.dot(np.linalg.inv(np.dot(x.T,x)),np.dot(x.T,y.T))
  return w

# defining gradient function
def gradient(phi_x,y,w):
  N=phi_x.shape[1]
  j=w.shape[0]-1
  gradient=(-2/N)*np.sum((y-np.dot(w.T,phi_x))*phi_x,axis=1).reshape(j+1,1)
  return gradient

def gradient_descent(w_initial,phi_x,y,alpha,max_iterations):
  N=phi_x.shape[1]
  w=w_initial
  iteration=0
  weights_vec=[]
  iterations=[]
  weights_vec.append(w_initial)
  iterations.append(0)
  while iteration<=max_iterations:
    dj_dw=gradient(phi_x,y,w)/N
    prev_cost=cost(phi_x,y,w)
    w=w-alpha*dj_dw
    
    curr_cost=cost(phi_x,y,w)
    epsilon=abs(curr_cost-prev_cost)
    # print(epsilon,end=",")
    if epsilon<=1e-5: # change in cost 
      break
    iteration=iteration+1
    weights_vec.append(w)
    iterations.append(iteration)
  return w,weights_vec,iterations

j_arr=np.arange(0,11,1)
N=50
x=np.linspace(-10,10,N)
x=x.reshape(1,x.size)
y=x+2*np.sin(1.5*x)+np.random.normal(0,2,50)

# learning rates for each j, adjusted to prevent overflow in calcuations
# alphas=[0.01,0.01,0.01,0.0001,1e-6,1e-8,1e-10,1e-12,1e-14,1e-16,1e-18]
alphas=[0.01,0.01,1e-7,1e-7,1e-10,1e-10,1e-13,1e-13,1e-13,1e-15,1e-16]
w_vec=[]
phix_vec=[]

for j in j_arr:
  w=np.zeros((j+1,1))
  phix=phi_j(x,j)
  w=get_weights(phix,y)
  wf,_,_=gradient_descent(w,phix,y,alphas[j],1000)
  w_vec.append(wf)
  phix_vec.append(phix)

fig,ax=plt.subplots(4,3,figsize=(15,15))
fig.tight_layout(pad=3)
for j in j_arr:
  ax[j//3][j%3].plot(x.T,np.dot(w_vec[j].T,phix_vec[j]).T)
  ax[j//3][j%3].scatter(x,y)
  ax[j//3][j%3].set_title(f"j = {j} ,  alpha = {alphas[j]}")  
  print(j,cost(phix_vec[j],y,w_vec[j]))
plt.show()

"""Gaussian Function"""

def phi_gaussian(x,mu,sigma):
  return np.exp(-np.square(x-mu)/(2*sigma*sigma))

N=50
x=np.linspace(-10,10,N)
x=x.reshape(1,x.size)
y=x+2*np.sin(1.5*x)+np.random.normal(0,2,N)
mu_vec=   [0,2,2,5,5,5, 10,10,20,20]
sigma_vec=[1,1,5,1,5,10,10,15,15,20]

w_vec=[]
phix_vec=[]

for i in range(10):
  w=np.zeros((2,1))
  phix=np.vstack((np.ones((1,N)),phi_gaussian(x,mu_vec[i],sigma_vec[i])))
  # phix=phi_gaussian(x_bias,10,5)
  # print(phix)
  wf,_,_=gradient_descent(w,phix,y,2,1000)
  w_vec.append(wf)
  phix_vec.append(phix)

fig,ax=plt.subplots(3,4,figsize=(15,10))
fig.tight_layout(pad=5)
for i in range(10):
  ax[i//4][i%4].plot(x.T,np.dot(w_vec[i].T,phix_vec[i]).T)
  ax[i//4][i%4].scatter(x,y)
  ax[i//4][i%4].set_title(f"mu = {mu_vec[i]}, sigma = {sigma_vec[i]}")
plt.show()

"""6. Addressing overfitting and regularization"""

def cost(phi_x,y,w,Lambda):
  N=phi_x.shape[1]
  # print(w.T.shape,phi_x.shape,np.dot(w.T,phi_x).shape,y.shape)
  # print(np.max(w))
  cost=np.sum(np.square(y-np.dot(w.T,phi_x)))/N+Lambda*np.linalg.norm(w)
  return cost

def gradient(phi_x,y,w,Lambda):
  N=phi_x.shape[1]
  j=w.shape[0]-1
  gradient=(-2/N)*np.sum((y-np.dot(w.T,phi_x))*phi_x,axis=1).reshape(j+1,1)+Lambda
  return gradient

def gradient_descent(w_initial,phi_x,y,alpha,max_iterations,Lambda):
  N=phi_x.shape[1]
  w=w_initial
  iteration=0
  weights_vec=[]
  iterations=[]
  weights_vec.append(w_initial)
  iterations.append(0)
  while iteration<=max_iterations:
    # if(iteration%100==0):
    #   print(iteration,Lambda)
    dj_dw=gradient(phi_x,y,w,Lambda)/N
    prev_cost=cost(phi_x,y,w,Lambda)
    w=w-alpha*dj_dw
    
    curr_cost=cost(phi_x,y,w,Lambda)
    epsilon=abs(curr_cost-prev_cost)
    # print(epsilon,end=",")
    if epsilon<=1e-5: # change in cost 
      break
    iteration=iteration+1
    weights_vec.append(w)
    iterations.append(iteration)
  return w,weights_vec,iterations

j=10 

N=50
x=np.linspace(-10,10,N)
x=x.reshape(1,x.size)
y=x+2*np.sin(1.5*x)+np.random.normal(0,2,50)

alpha=1e-18

L_vec=[0.25,0.5,0.75,1,1.25]
L=5
w_vec=[]

phix=phi_j(x,j)

for l in range(L):
  w=get_weights(phix,y)
  # w=np.zeros((j+1,1))
  # w=np.random.randn(j+1,1)
  wf,_,_=gradient_descent(w,phix,y,alpha,1000,L_vec[l])
  w_vec.append(wf)

fig,ax=plt.subplots(2,3,figsize=(15,10))
fig.tight_layout(pad=3)
for l in range(L):
  ax[l//3][l%3].plot(x.T,np.dot(w_vec[l].T,phix).T)
  ax[l//3][l%3].scatter(x,y)
  ax[l//3][l%3].set_title(f"j = {j} ,  l = {L_vec[l]}")  
  print(L_vec[l] ,cost(phix,y,w_vec[l],L_vec[l]))
# print(w_vec[8])
plt.show()

x_test=np.linspace(-5,5,50)
x_test=x_test.reshape(1,x.size)
y_test=x_test+2*np.sin(1.5*x_test)+np.random.normal(0,2,50)

j=10
gen_err=[]
training_err=[]
capacity=[]
for l in range(L):
  capacity.append(l)
  phix_test=phi_j(x_test,j)
  gen_err.append(cost(phix_test,y_test,w_vec[l],L_vec[l]))
  training_err.append(cost(phix,y,w_vec[l],L_vec[l]))

plt.plot(capacity,gen_err,label='generalization error')
plt.plot(capacity,training_err,label='training error')
plt.legend()
plt.show()

def cost(phi_x,y,w,Lambda):
  N=phi_x.shape[1]
  # print(w.T.shape,phi_x.shape,np.dot(w.T,phi_x).shape,y.shape)
  cost=np.sum(np.square(y-np.dot(w.T,phi_x)))/N+Lambda*(np.linalg.norm(w)**2)
  return cost

def gradient(phi_x,y,w,Lambda):
  N=phi_x.shape[1]
  j=w.shape[0]-1
  gradient=(-2/N)*np.sum((y-np.dot(w.T,phi_x))*phi_x,axis=1).reshape(j+1,1)+Lambda*np.linalg.norm(w)
  return gradient

j=10 

N=50
x=np.linspace(-10,10,N)
x=x.reshape(1,x.size)
y=x+2*np.sin(1.5*x)+np.random.normal(0,2,50)

alpha=1e-18

L_vec=[0.25,0.5,0.75,1,1.25]
L=5
w_vec=[]

phix=phi_j(x,j)

for l in L_vec:
  w=get_weights(phix,y)
  # w=np.zeros((j+1,1))
  wf,_,_=gradient_descent(w,phix,y,alpha,1000,l)
  w_vec.append(wf)

fig,ax=plt.subplots(2,3,figsize=(15,10))
fig.tight_layout(pad=3)
for l in range(L):
  ax[l//3][l%3].plot(x.T,np.dot(w_vec[l].T,phix).T)
  ax[l//3][l%3].scatter(x,y)
  ax[l//3][l%3].set_title(f"j = {j} ,  l = {L_vec[l]}")  
  print(L_vec[l] ,cost(phix,y,w_vec[l],L_vec[l]))
# print(w_vec[8])
plt.show()

x_test=np.linspace(-5,5,50)
x_test=x_test.reshape(1,x.size)
y_test=x_test+2*np.sin(1.5*x_test)+np.random.normal(0,2,50)

j=10
gen_err=[]
training_err=[]
capacity=[]
for l in range(L):
  capacity.append(l)
  phix_test=phi_j(x_test,j)
  gen_err.append(cost(phix_test,y_test,w_vec[l],L_vec[l]))
  training_err.append(cost(phix,y,w_vec[l],L_vec[l]))

plt.plot(capacity,gen_err,label='generalization error')
plt.plot(capacity,training_err,label='training error')
plt.legend()
plt.show()